{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Quantitative evaluation script for discovery algorithms and conformance checking methods using four different dimensions for log-model comparison**"
      ],
      "metadata": {
        "id": "F135xvQ-2YiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following script evaluates secutiry-relevant datasets using a cross-validation approach. In practice, manual splitting by certain criteria might be more valuable, but the goal of the evaluation is discussion from the quantitative perspective, observing differences between algorithms an conformance checking methods accross different dimensions.\n",
        "\n",
        "The applied method of k-fold cross validation for process mining is in line with the framework in Rozinat, Anne & Medeiros, A & GÃ¼nther, C & Weijters, A. & Aalst, Wil. (2007). Towards an evaluation framework for process mining algorithms."
      ],
      "metadata": {
        "id": "qbV7LYVW5Ukh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note:these datasets are meant to support quantitative evaluation only, for case studies, different data sets are offered in the repository."
      ],
      "metadata": {
        "id": "Hi4KM-m95P73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's install the dependencies that we will require"
      ],
      "metadata": {
        "id": "u3klTJMv5NL4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu-tML_HhsPX"
      },
      "outputs": [],
      "source": [
        "# Install PM4Py\n",
        "!pip install pm4py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u13MH_2agCyr"
      },
      "outputs": [],
      "source": [
        "# Import XES log converting functionality from PM4Py\n",
        "from pm4py.objects.conversion.log import converter as xes_converter\n",
        "# Import XES log importing functionality from PM4Py\n",
        "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
        "from pm4py.algo.filtering.pandas.cases import case_filter\n",
        "\n",
        "# Import discovery algorithms from PM4Py\n",
        "from pm4py.algo.discovery.alpha import algorithm as alpha_miner\n",
        "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
        "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
        "from pm4py.algo.discovery.inductive.variants.im_clean.algorithm import Parameters\n",
        "from pm4py.objects.conversion.process_tree import converter\n",
        "\n",
        "# Import conformance checking algorithms from PM4Py\n",
        "from pm4py.algo.conformance.tokenreplay import algorithm as token_replay\n",
        "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
        "\n",
        "# Import quality evaluation algorithms from PM4py\n",
        "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
        "from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator\n",
        "from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
        "from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator\n",
        "\n",
        "\n",
        "# Import helper functionalities\n",
        "from pm4py.objects.log.obj import EventLog\n",
        "from pm4py.util.xes_constants import KEY_NAME\n",
        "from typing import Tuple\n",
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "import pm4py\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, load the datasets tha will be used for testing"
      ],
      "metadata": {
        "id": "7E5z90wCQGo0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpTIx3O5g-gJ"
      },
      "outputs": [],
      "source": [
        "# Load datasets, please adjust path to each file, depeding on where you run the notebook, Dataset can be find in the GitHUb of the submission, or sourced from their respective sources\n",
        "datasets = {}\n",
        "\n",
        "\n",
        "# The following is a brief description of the data sets\n",
        "\n",
        "# Dataset 1: Log of Volvo IT incident management system Parent item: BPI Challenge 2013 Logs of Volvo IT incident and problem management\n",
        "# Source of helpdesk dataset: BPI Challenge 2013, https://data.4tu.nl/articles/dataset/BPI_Challenge_2013_incidents/12693914/1\n",
        "#  http://www.win.tue.nl/bpi/2013/challenge\n",
        "datasets['bpi_challenge_2013_incidents'] = xes_importer.apply('bpi_challenge_2013_incidents.xes')\n",
        "\n",
        "# Dataset 2: Log of Volvo IT problem management system\n",
        "# Source of helpdesk dataset: BPI Challenge 2013, https://www.win.tue.nl/bpi/doku.php?id=2013:challenge&redirect=1id=2013/challenge\n",
        "# Reference: http://www.win.tue.nl/bpi/2013/challenge\n",
        "datasets['BPI_Challenge_2013_closed_problems'] = xes_importer.apply('dBPI_Challenge_2013_closed_problems.xes')\n",
        "\n",
        "\n",
        "# Dataset 3:  Dutch Financial Institute log is an application process for a personal loan or overdraft\n",
        "# Source of helpdesk dataset: BPI Challenge 201,  Dutch Financial Institute\n",
        "# Reference: https://www.win.tue.nl/bpi/doku.php?id=2012:challenge\n",
        "\n",
        "datasets['BPI_Challenge_2012_Complete'] = xes_importer.apply('BPI_Challenge_2012_Complete.xes.')\n",
        "\n",
        "\n",
        "# Dataset 4: Log of a helpdesk process\n",
        "# Source of helpdesk dataset: Public Repository of Process Mining Datasest, https://github.com/ERamaM/ProcessMiningDatasets/tree/master/XES\n",
        "# Import\n",
        "datasets['Helpdesk'] = xes_importer.apply('Helpdesk.xes')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preview"
      ],
      "metadata": {
        "id": "eqoGjYrWQQim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview datasets in a pandas datafame to get an overview\n",
        "for key, ds in datasets.items():\n",
        "  print(f\"Loaded Dataset: {key}\")\n",
        "  df = xes_converter.apply(ds, variant=xes_converter.Variants.TO_DATA_FRAME)\n",
        "  display(df)\n",
        "  print(\"\\r\\n\\r\\n\\r\\n\")"
      ],
      "metadata": {
        "id": "NiaJDQ7fb9wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nab1aW5FQTGN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI9-rTCYp_Zg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def split(log: EventLog, test_ds_idxs = list()) -> Tuple[EventLog, EventLog]:\n",
        "    \"\"\"\n",
        "    Split the event log in a discovery log (on which algorithm is applied to discover a model, corresponds to train) and a test log, which is then replayed in/aligned with the discovered model.\n",
        "    test_ds_idxs : list the case ids in the log Please note splitting on cases is applied, not on events.\n",
        "    This function will slipt the log to train and test dataset based on test dataset ids of case.\n",
        "    \n",
        "    \"\"\"\n",
        "    idxs_test = []\n",
        "    idxs_train = []\n",
        "\n",
        "    print(test_ds_idxs)\n",
        "    for i in range(len(log)):\n",
        "      if i in test_ds_idxs:\n",
        "        idxs_test.append(i)\n",
        "      else:\n",
        "        idxs_train.append(i)\n",
        "\n",
        "    train_log = EventLog(list(), attributes=log.attributes, extensions=log.extensions, classifiers=log.classifiers,\n",
        "                            omni_present=log.omni_present, properties=log.properties)\n",
        "    test_log = EventLog(list(), attributes=log.attributes, extensions=log.extensions, classifiers=log.classifiers,\n",
        "                            omni_present=log.omni_present, properties=log.properties)\n",
        "    for idx in idxs_train:\n",
        "        train_log.append(log[idx])\n",
        "    for idx in idxs_test:\n",
        "        test_log.append(log[idx])\n",
        "    return train_log, test_log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This can be used to adjust the parameters to be used for testing across the various algorithms, conformance checking, evaluation, and k-fold validation\n",
        "'''\n",
        "You can also define the K for the k-fold cross validation. For example, 5 fold cross validation.\n",
        "\n",
        "'''\n",
        "params = {\n",
        "    'k-fold' : 5,\n",
        "    'algo': ['Alpha', 'Inductive', 'Heuristic'], # deifne the process mining algorithms to discover model with \n",
        "    'conformance' : ['Token-based replay', 'Alignments'], # conformance checking methods to apply\n",
        "    'evaluation' : ['Fitness', 'Precision', 'Generalization', 'Simplicity'], # evaluation metrics to calculate\n",
        "}"
      ],
      "metadata": {
        "id": "iynwEYUP0KvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crossvalidation_algotest calculates the quality metrics across the various algorithms and conformance checking methods based on k-fold cross validation methods for a given dataset\n",
        "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
        "\n",
        "def crossvalidation_algotest(log: EventLog):\n",
        "  global params\n",
        "  # K for the k fold cross validation\n",
        "  k_fold = params['k-fold']\n",
        "\n",
        "  # Define which discovery algorithms to use for model discovery (rougly corresponds to train)\n",
        "  algorithms = params['algo']\n",
        "\n",
        "  # Define what conformance checking methods will be used\n",
        "  conformance_checking = params['conformance']\n",
        "\n",
        "  # Define what evaluation methods will be used\n",
        "  evaluations = params['evaluation']\n",
        "\n",
        "  # Get the total number of event cases from the loaded event\n",
        "  cnt_logs = len(log)\n",
        "  print(f'Total number of cases in the loaded event log is {cnt_logs}')\n",
        "\n",
        "  # Calculate the number of test dataset cases based on k-fold parameter setting\n",
        "  cnt_test = math.floor(cnt_logs / k_fold)\n",
        "\n",
        "\n",
        "  outputs = []\n",
        "  # Put together all algorithms \n",
        "  for k in range(len(algorithms)):\n",
        "    algo = algorithms[k]\n",
        "    output = []\n",
        "    label = []\n",
        "\n",
        "    # Put together both conformance checking methods\n",
        "    for conformance in conformance_checking:\n",
        "      for evaluation in evaluations:\n",
        "        metrics = 0\n",
        "\n",
        "        # apply k-fold cross validation\n",
        "        for i in range(k_fold):\n",
        "          #split the dataset into train(discover)/test(replay) one\n",
        "          end_idx = int(cnt_logs - cnt_test * i)\n",
        "          start_idx = int(end_idx - cnt_test)\n",
        "          #print(f\"start idx {start_idx}, end_idx {end_idx}\")\n",
        "          test_ds_idxs = sorted(range(start_idx, end_idx))\n",
        "          print(f'Number of cases in test event log is {len(test_ds_idxs)}')\n",
        "          train_ds, test_ds = split(log, test_ds_idxs)\n",
        "          #test_ds = log\n",
        "\n",
        "          #df_train = xes_converter.apply(train_ds, variant=xes_converter.Variants.TO_DATA_FRAME)\n",
        "          #df_test = xes_converter.apply(test_ds, variant=xes_converter.Variants.TO_DATA_FRAME)\n",
        "          #display(df_train)\n",
        "          #display(df_test)\n",
        "\n",
        "          if algo.lower() == \"alpha\":\n",
        "            net, initial_marking, final_marking = alpha_miner.apply(train_ds)\n",
        "          elif algo.lower() == \"inductive\":\n",
        "            ptree = inductive_miner.apply_tree(train_ds, parameters={Parameters.NOISE_THRESHOLD: 0.2}, variant=inductive_miner.Variants.IM_CLEAN)\n",
        "            net, initial_marking, final_marking = pt_converter.apply(ptree, variant=pt_converter.Variants.TO_PETRI_NET)#converter.apply(ptree)\n",
        "            #net, initial_marking, final_marking = inductive_miner.apply(train_ds)\n",
        "          else: # elif algo.lower() == \"heuristic\":\n",
        "            #net, initial_marking, final_marking = heuristics_miner.apply(train_ds, parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.99})\n",
        "            net, initial_marking, final_marking = heuristics_miner.apply(train_ds, parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.5})\n",
        "\n",
        "          is_sound = pm4py.objects.petri_net.utils.check_soundness.check_easy_soundness_net_in_fin_marking(net, initial_marking, final_marking)\n",
        "          print(f'Is easy soundness : {is_sound}')\n",
        "          #print(conformance)\n",
        "          try:\n",
        "            if conformance.lower() == \"token-based replay\":\n",
        "              if evaluation.lower() == \"fitness\":\n",
        "                fitness = replay_fitness_evaluator.apply(test_ds, net, initial_marking, final_marking, variant=replay_fitness_evaluator.Variants.TOKEN_BASED)\n",
        "                print(fitness)\n",
        "                metrics = metrics + fitness['average_trace_fitness']\n",
        "              elif evaluation.lower() == \"precision\":\n",
        "                metrics = metrics + precision_evaluator.apply(test_ds, net, initial_marking, final_marking, variant=precision_evaluator.Variants.ETCONFORMANCE_TOKEN)\n",
        "              elif evaluation.lower() == \"generalization\":\n",
        "                metrics = metrics + generalization_evaluator.apply(test_ds, net, initial_marking, final_marking)\n",
        "              else: #Simplicity\n",
        "                metrics = metrics + simplicity_evaluator.apply(net)\n",
        "            elif conformance.lower() == \"alignments\":\n",
        "              if evaluation.lower() == \"fitness\":\n",
        "                fitness = replay_fitness_evaluator.apply(test_ds, net, initial_marking, final_marking, variant=replay_fitness_evaluator.Variants.ALIGNMENT_BASED)\n",
        "                print(fitness)\n",
        "                metrics = metrics + fitness['average_trace_fitness']\n",
        "              elif evaluation.lower() == \"precision\":\n",
        "                try:\n",
        "                  metrics = metrics + precision_evaluator.apply(test_ds, net, initial_marking, final_marking, variant=precision_evaluator.Variants.ALIGN_ETCONFORMANCE)\n",
        "                except:\n",
        "                  metrics = math.nan\n",
        "              elif evaluation.lower() == \"generalization\":\n",
        "                metrics = metrics + generalization_evaluator.apply(test_ds, net, initial_marking, final_marking)\n",
        "              else: #Simplicity\n",
        "                metrics = metrics + simplicity_evaluator.apply(net)\n",
        "          except:\n",
        "            print(f'Excpetion occured {algo}/{conformance}')\n",
        "            metrics = math.nan\n",
        "        \n",
        "        metrics = metrics / k_fold\n",
        "        label.append(conformance + \"/\" + evaluation)\n",
        "        output.append(metrics)\n",
        "    \n",
        "    if k == 0:\n",
        "      outputs.append(label)\n",
        "    outputs.append(output)\n",
        "\n",
        "  print(outputs)\n",
        "  return outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "SvM5bt0mZoW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The relevant quality metrics based on parameters for all event logs selected\n",
        "result = {} # the final resuls is a disctionary with a dataframe containing metrics vs event log name\n",
        "for key, ds in datasets.items():\n",
        "  metrics = crossvalidation_algotest(ds)\n",
        "  algorithms = parameters['algo']\n",
        "  df = pd.DataFrame()\n",
        "  df['Conformance checking'] = metrics[0]\n",
        "  for i in range(len(algorithms)):\n",
        "    algo = algorithms[i]\n",
        "    df[algo] = metrics[i + 1]\n",
        "  \n",
        "  result[key] = df"
      ],
      "metadata": {
        "id": "Z8isUdsXeNez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display table with values of quality metrics\n",
        "for key, res in result.items():\n",
        "  print(f\"The result of '{key}' event log\")\n",
        "  display(res)"
      ],
      "metadata": {
        "id": "Z9fK-WaCgPuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot bar chars of process mining quality metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def overlapped_bar(df, show=False, width=0.25, alpha=1,\n",
        "                   title='', xlabel='', ylabel='', **plot_kwargs):\n",
        "    plt.rcParams['figure.figsize'] = (24, 5)\n",
        "    xlabel = xlabel or df.index.name\n",
        "    N = len(df)\n",
        "    M = len(df.columns)\n",
        "    indices = np.arange(N)\n",
        "    #colors = ['steelblue', 'firebrick', 'darksage', 'goldenrod', 'gray'] * int(M / 5. + 1)\n",
        "    colors = ['green', 'blue', 'red']\n",
        "    for i, label, color in zip(range(M), df.columns, colors):\n",
        "      kwargs = plot_kwargs\n",
        "      kwargs.update({'color': color, 'label': label})\n",
        "      plt.bar(indices + width * (i - 1), df[label], width=width, alpha=alpha if i else 1, **kwargs)\n",
        "      plt.xticks(indices,\n",
        "                  ['{}'.format(idx) for idx in df.index.values])\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.grid()\n",
        "    if show:\n",
        "        plt.show()\n",
        "    return plt.gcf()\n",
        "\n",
        "for key, res in result.items():\n",
        "  print(f\"\\nOverlay bar chart with evaluation metrics vs. conformance checking methods for '{key}' event log\")\n",
        "  df = res.set_index('Conformance checking')\n",
        "  overlapped_bar(df, show=True, title = f'{key}')\n"
      ],
      "metadata": {
        "id": "i2CJgsqmoETu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Quantative_evaluation_implementation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}